{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmasti/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    small_model = \"EleutherAI/pythia-410m\"\n",
    "    large_model = \"EleutherAI/pythia-1.4b\"\n",
    "    small_adapter = \"./models/raw/pythia_410m_r=8_0.0001_gsm8k\"\n",
    "    rank = 8\n",
    "    expanded_model = \"./models/expanded/pythia_410m_1.4b_r=8_0.0001_gsm8k\"\n",
    "    lora_alpha = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = GPTNeoXForCausalLM.from_pretrained(args.small_model)\n",
    "model_large = GPTNeoXForCausalLM.from_pretrained(args.large_model)\n",
    "\n",
    "model_small.load_adapter(args.small_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer A: Parameter containing:\n",
      "tensor([[-0.0450,  0.0096, -0.0191,  ..., -0.0397,  0.0226, -0.0111],\n",
      "        [-0.0042,  0.0038, -0.0143,  ...,  0.0165,  0.0095,  0.0102],\n",
      "        [ 0.0273,  0.0124,  0.0265,  ...,  0.0227,  0.0082, -0.0271],\n",
      "        ...,\n",
      "        [ 0.0103,  0.0151,  0.0282,  ...,  0.0160, -0.0006,  0.0154],\n",
      "        [ 0.0276, -0.0378,  0.0052,  ...,  0.0096,  0.0358, -0.0144],\n",
      "        [ 0.0390, -0.0173,  0.0081,  ...,  0.0212,  0.0357,  0.0103]])\n",
      "First layer B: Parameter containing:\n",
      "tensor([[ 0.0136,  0.0021, -0.0162,  ..., -0.0092,  0.0207,  0.0147],\n",
      "        [-0.0034,  0.0105,  0.0107,  ...,  0.0013, -0.0084, -0.0162],\n",
      "        [-0.0090, -0.0129, -0.0078,  ...,  0.0048,  0.0092, -0.0026],\n",
      "        ...,\n",
      "        [-0.0036,  0.0067,  0.0023,  ..., -0.0013,  0.0042, -0.0018],\n",
      "        [ 0.0036,  0.0010, -0.0139,  ..., -0.0004,  0.0097,  0.0135],\n",
      "        [-0.0016,  0.0130, -0.0097,  ..., -0.0022,  0.0044, -0.0125]])\n",
      "Weights are not tied for gpt_neox.layers.1.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0373, -0.0103, -0.0286,  ..., -0.0446,  0.0134, -0.0106],\n",
      "        [-0.0316, -0.0312,  0.0180,  ...,  0.0376, -0.0406, -0.0217],\n",
      "        [-0.0029,  0.0272, -0.0457,  ...,  0.0093, -0.0216,  0.0183],\n",
      "        ...,\n",
      "        [-0.0346, -0.0194,  0.0187,  ...,  0.0527, -0.0121,  0.0030],\n",
      "        [-0.0082,  0.0416, -0.0183,  ...,  0.0089, -0.0285, -0.0038],\n",
      "        [-0.0279,  0.0046,  0.0363,  ...,  0.0212,  0.0143, -0.0300]])\n",
      "Weights are not tied for gpt_neox.layers.1.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0088, -0.0194, -0.0031,  ...,  0.0062,  0.0139, -0.0080],\n",
      "        [-0.0091,  0.0120, -0.0124,  ...,  0.0061,  0.0101,  0.0161],\n",
      "        [ 0.0006, -0.0143,  0.0055,  ..., -0.0026, -0.0028, -0.0133],\n",
      "        ...,\n",
      "        [ 0.0106,  0.0049,  0.0011,  ...,  0.0088, -0.0049,  0.0111],\n",
      "        [-0.0033, -0.0094,  0.0087,  ...,  0.0098,  0.0062,  0.0034],\n",
      "        [ 0.0087,  0.0017, -0.0056,  ..., -0.0010, -0.0012, -0.0060]])\n",
      "Weights are not tied for gpt_neox.layers.2.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0491,  0.0392,  0.0228,  ...,  0.0203, -0.0274,  0.0189],\n",
      "        [ 0.0211, -0.0055, -0.0248,  ...,  0.0156, -0.0146,  0.0215],\n",
      "        [-0.0118,  0.0287, -0.0458,  ...,  0.0206,  0.0260, -0.0492],\n",
      "        ...,\n",
      "        [-0.0298, -0.0028,  0.0374,  ...,  0.0292,  0.0133, -0.0439],\n",
      "        [ 0.0040,  0.0061,  0.0031,  ..., -0.0322,  0.0198, -0.0201],\n",
      "        [-0.0469,  0.0107,  0.0185,  ...,  0.0322,  0.0312,  0.0158]])\n",
      "Weights are not tied for gpt_neox.layers.2.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 7.1636e-03, -8.5871e-03,  1.3002e-02,  ..., -4.7242e-03,\n",
      "         -3.6219e-03,  5.2527e-03],\n",
      "        [ 4.0395e-03,  2.4097e-03, -1.1072e-02,  ...,  1.4456e-02,\n",
      "         -7.1350e-03, -1.5205e-02],\n",
      "        [-9.0497e-03, -1.6218e-03, -7.4794e-03,  ..., -1.1394e-02,\n",
      "         -2.3526e-03, -1.6471e-02],\n",
      "        ...,\n",
      "        [ 2.3169e-03,  2.4543e-03, -3.7718e-03,  ...,  4.2077e-03,\n",
      "         -3.1461e-04, -8.1640e-03],\n",
      "        [ 4.9402e-03, -3.2843e-03,  4.0454e-03,  ..., -1.7647e-03,\n",
      "          1.2824e-03,  7.5853e-04],\n",
      "        [ 2.9716e-03, -3.7532e-03, -4.3431e-03,  ..., -9.5222e-05,\n",
      "          1.2780e-03, -6.7558e-03]])\n",
      "Weights are not tied for gpt_neox.layers.3.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0094, -0.0141,  0.0061,  ..., -0.0287, -0.0487,  0.0548],\n",
      "        [-0.0366,  0.0322, -0.0072,  ..., -0.0106,  0.0036, -0.0247],\n",
      "        [ 0.0279, -0.0252, -0.0141,  ..., -0.0215,  0.0114, -0.0225],\n",
      "        ...,\n",
      "        [ 0.0326,  0.0292, -0.0399,  ..., -0.0159,  0.0258, -0.0030],\n",
      "        [-0.0198, -0.0290,  0.0131,  ...,  0.0119, -0.0340,  0.0228],\n",
      "        [ 0.0197, -0.0383,  0.0144,  ..., -0.0424,  0.0062,  0.0473]])\n",
      "Weights are not tied for gpt_neox.layers.3.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-7.6447e-04,  9.8857e-03, -1.7691e-02,  ..., -1.9551e-02,\n",
      "         -8.8595e-03,  1.5929e-02],\n",
      "        [ 5.7933e-03, -5.2009e-03, -1.0256e-02,  ..., -1.4967e-02,\n",
      "         -7.3252e-03,  1.5375e-02],\n",
      "        [ 1.0725e-03, -1.0871e-02, -2.3430e-03,  ..., -6.5093e-03,\n",
      "         -3.7952e-03, -5.8451e-03],\n",
      "        ...,\n",
      "        [ 6.0006e-03,  4.6767e-04, -3.0944e-03,  ...,  1.1209e-02,\n",
      "         -4.7764e-03,  3.9518e-05],\n",
      "        [ 6.4881e-03,  5.9076e-03, -2.7274e-04,  ..., -4.3972e-03,\n",
      "         -1.2727e-02,  4.6626e-03],\n",
      "        [-1.0883e-02,  2.4994e-04,  1.9678e-03,  ...,  1.0455e-02,\n",
      "          6.4810e-03,  4.3319e-03]])\n",
      "Weights are not tied for gpt_neox.layers.4.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0136,  0.0259,  0.0243,  ..., -0.0464, -0.0103, -0.0099],\n",
      "        [-0.0217, -0.0073, -0.0078,  ...,  0.0072,  0.0291, -0.0233],\n",
      "        [-0.0308,  0.0167, -0.0277,  ..., -0.0114,  0.0247, -0.0268],\n",
      "        ...,\n",
      "        [ 0.0121, -0.0117, -0.0304,  ..., -0.0693,  0.0283, -0.0013],\n",
      "        [-0.0395,  0.0104, -0.0260,  ..., -0.0514, -0.0184, -0.0065],\n",
      "        [-0.0244,  0.0156, -0.0003,  ...,  0.0069, -0.0049,  0.0123]])\n",
      "Weights are not tied for gpt_neox.layers.4.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0077, -0.0030,  0.0135,  ...,  0.0080, -0.0086, -0.0090],\n",
      "        [ 0.0002, -0.0003,  0.0014,  ..., -0.0023, -0.0088,  0.0003],\n",
      "        [-0.0015, -0.0014,  0.0039,  ...,  0.0003, -0.0045, -0.0014],\n",
      "        ...,\n",
      "        [-0.0066,  0.0087, -0.0121,  ..., -0.0029, -0.0128,  0.0105],\n",
      "        [ 0.0168,  0.0076, -0.0051,  ...,  0.0053, -0.0021,  0.0194],\n",
      "        [ 0.0034,  0.0023, -0.0046,  ..., -0.0031, -0.0105, -0.0013]])\n",
      "Weights are not tied for gpt_neox.layers.5.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0364,  0.0182, -0.0206,  ...,  0.0311,  0.0430, -0.0006],\n",
      "        [-0.0125,  0.0053, -0.0384,  ...,  0.0016, -0.0400,  0.0026],\n",
      "        [ 0.0139, -0.0072,  0.0023,  ..., -0.0235,  0.0206,  0.0137],\n",
      "        ...,\n",
      "        [ 0.0169,  0.0426, -0.0283,  ...,  0.0107, -0.0005, -0.0202],\n",
      "        [ 0.0194,  0.0528, -0.0019,  ...,  0.0454, -0.0224, -0.0115],\n",
      "        [-0.0159,  0.0028, -0.0365,  ..., -0.0372,  0.0037, -0.0685]])\n",
      "Weights are not tied for gpt_neox.layers.5.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0079, -0.0108, -0.0029,  ...,  0.0063,  0.0065,  0.0065],\n",
      "        [-0.0044, -0.0069,  0.0048,  ..., -0.0013, -0.0089, -0.0136],\n",
      "        [-0.0064, -0.0063, -0.0121,  ...,  0.0055, -0.0032,  0.0083],\n",
      "        ...,\n",
      "        [-0.0026, -0.0050,  0.0019,  ...,  0.0094,  0.0125,  0.0108],\n",
      "        [ 0.0076,  0.0115,  0.0132,  ..., -0.0063, -0.0107, -0.0070],\n",
      "        [ 0.0005, -0.0010, -0.0057,  ...,  0.0014,  0.0001,  0.0074]])\n",
      "Weights are not tied for gpt_neox.layers.6.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0315, -0.0190, -0.0545,  ..., -0.0111, -0.0111,  0.0261],\n",
      "        [ 0.0110,  0.0352, -0.0179,  ...,  0.0142,  0.0140,  0.0163],\n",
      "        [-0.0338, -0.0220, -0.0143,  ..., -0.0053,  0.0492, -0.0132],\n",
      "        ...,\n",
      "        [ 0.0050, -0.0369, -0.0292,  ...,  0.0172,  0.0338,  0.0008],\n",
      "        [-0.0235, -0.0131,  0.0096,  ..., -0.0160, -0.0103,  0.0085],\n",
      "        [ 0.0102,  0.0308, -0.0168,  ..., -0.0351, -0.0119, -0.0481]])\n",
      "Weights are not tied for gpt_neox.layers.6.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-6.7605e-03, -7.5403e-03, -5.4446e-03,  ..., -7.8869e-03,\n",
      "          2.5660e-04,  6.8042e-03],\n",
      "        [ 5.0812e-03, -7.4642e-03,  1.5601e-02,  ...,  1.7118e-02,\n",
      "          2.3141e-02, -2.0105e-02],\n",
      "        [ 3.2585e-03, -2.1648e-03, -5.2116e-04,  ..., -4.8674e-03,\n",
      "          1.0136e-02,  2.7248e-05],\n",
      "        ...,\n",
      "        [ 2.7036e-03,  6.0374e-03,  3.0003e-03,  ...,  1.1540e-04,\n",
      "          6.0291e-03, -3.9259e-03],\n",
      "        [ 5.4872e-04, -6.5795e-03,  5.1448e-03,  ...,  1.6898e-03,\n",
      "          7.7699e-03, -4.9017e-03],\n",
      "        [ 8.3327e-03, -8.3203e-03, -1.0032e-02,  ..., -1.2118e-04,\n",
      "          4.2173e-03,  1.5980e-03]])\n",
      "Weights are not tied for gpt_neox.layers.7.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0187,  0.0059,  0.0005,  ..., -0.0242,  0.0228, -0.0172],\n",
      "        [ 0.0414, -0.0187,  0.0529,  ...,  0.0397, -0.0501,  0.0269],\n",
      "        [-0.0242, -0.0353, -0.0358,  ...,  0.0173,  0.0280, -0.0172],\n",
      "        ...,\n",
      "        [-0.0149, -0.0304,  0.0300,  ..., -0.0204, -0.0045,  0.0019],\n",
      "        [ 0.0274, -0.0183,  0.0165,  ..., -0.0110, -0.0262,  0.0319],\n",
      "        [-0.0234, -0.0018, -0.0038,  ..., -0.0393, -0.0306, -0.0302]])\n",
      "Weights are not tied for gpt_neox.layers.7.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0102, -0.0195,  0.0007,  ..., -0.0036, -0.0009,  0.0015],\n",
      "        [-0.0076,  0.0032,  0.0017,  ...,  0.0011, -0.0026, -0.0026],\n",
      "        [ 0.0010,  0.0078, -0.0057,  ..., -0.0010, -0.0019,  0.0018],\n",
      "        ...,\n",
      "        [ 0.0059, -0.0045, -0.0059,  ..., -0.0050, -0.0041,  0.0001],\n",
      "        [ 0.0048, -0.0082,  0.0141,  ..., -0.0011, -0.0105, -0.0094],\n",
      "        [ 0.0002,  0.0050, -0.0053,  ...,  0.0008,  0.0055,  0.0037]])\n",
      "Weights are not tied for gpt_neox.layers.8.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0123,  0.0279, -0.0245,  ..., -0.0227, -0.0372, -0.0411],\n",
      "        [-0.0294, -0.0005,  0.0034,  ...,  0.0059, -0.0333, -0.0280],\n",
      "        [-0.0324, -0.0558,  0.0059,  ...,  0.0356, -0.0403, -0.0055],\n",
      "        ...,\n",
      "        [-0.0448, -0.0312, -0.0268,  ...,  0.0189, -0.0286, -0.0247],\n",
      "        [ 0.0005,  0.0204, -0.0041,  ...,  0.0126,  0.0201, -0.0096],\n",
      "        [ 0.0101,  0.0091, -0.0139,  ..., -0.0246, -0.0038, -0.0037]])\n",
      "Weights are not tied for gpt_neox.layers.8.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 2.4331e-03,  4.3989e-03,  1.6034e-02,  ...,  1.4756e-02,\n",
      "          9.0062e-03,  1.4616e-02],\n",
      "        [-9.4672e-03, -5.3710e-03, -4.6500e-04,  ..., -8.5191e-05,\n",
      "          1.3823e-02, -2.7918e-03],\n",
      "        [ 5.4873e-04, -2.2568e-03,  1.4947e-03,  ..., -3.5131e-03,\n",
      "         -2.2155e-03, -8.1511e-04],\n",
      "        ...,\n",
      "        [-2.6233e-04,  7.7078e-04,  3.9899e-03,  ..., -9.3597e-03,\n",
      "          1.4491e-02,  4.4035e-03],\n",
      "        [ 4.5100e-03, -1.7418e-03, -6.8800e-03,  ..., -7.0020e-03,\n",
      "         -1.9648e-02, -4.2895e-04],\n",
      "        [-3.0860e-03,  1.3591e-02, -3.2488e-03,  ..., -1.8301e-03,\n",
      "          5.0738e-03,  1.4352e-02]])\n",
      "Weights are not tied for gpt_neox.layers.9.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0293, -0.0181,  0.0109,  ..., -0.0143, -0.0098,  0.0042],\n",
      "        [ 0.0222, -0.0277,  0.0240,  ..., -0.0058, -0.0555, -0.0561],\n",
      "        [-0.0128,  0.0123, -0.0015,  ...,  0.0142, -0.0231, -0.0202],\n",
      "        ...,\n",
      "        [-0.0123,  0.0118,  0.0560,  ...,  0.0134, -0.0278, -0.0378],\n",
      "        [ 0.0051,  0.0102, -0.0157,  ...,  0.0085, -0.0218,  0.0305],\n",
      "        [-0.0482, -0.0145,  0.0247,  ...,  0.0073, -0.0132,  0.0110]])\n",
      "Weights are not tied for gpt_neox.layers.9.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0263, -0.0372,  0.0240,  ...,  0.0132, -0.0016,  0.0216],\n",
      "        [-0.0031,  0.0100,  0.0051,  ...,  0.0285,  0.0067, -0.0005],\n",
      "        [-0.0182, -0.0009,  0.0018,  ...,  0.0206,  0.0104, -0.0193],\n",
      "        ...,\n",
      "        [ 0.0077,  0.0010,  0.0067,  ..., -0.0022,  0.0041,  0.0083],\n",
      "        [-0.0009,  0.0042, -0.0012,  ...,  0.0001,  0.0130,  0.0014],\n",
      "        [ 0.0037, -0.0014,  0.0015,  ..., -0.0046,  0.0022,  0.0067]])\n",
      "Weights are not tied for gpt_neox.layers.10.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0459, -0.0214, -0.0073,  ..., -0.0256,  0.0358, -0.0191],\n",
      "        [-0.0117,  0.0423,  0.0273,  ...,  0.0132,  0.0536, -0.0058],\n",
      "        [-0.0098, -0.0428,  0.0104,  ...,  0.0303,  0.0043,  0.0154],\n",
      "        ...,\n",
      "        [-0.0610,  0.0034,  0.0305,  ..., -0.0085, -0.0152,  0.0059],\n",
      "        [-0.0421,  0.0087, -0.0096,  ...,  0.0030, -0.0018, -0.0055],\n",
      "        [ 0.0596, -0.0270,  0.0245,  ..., -0.0174,  0.0146, -0.0132]])\n",
      "Weights are not tied for gpt_neox.layers.10.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-7.3611e-03,  5.9951e-03, -5.6963e-03,  ..., -1.1683e-02,\n",
      "          2.0358e-04, -9.4368e-03],\n",
      "        [-1.0811e-02, -8.9461e-03,  1.1516e-02,  ..., -2.5726e-02,\n",
      "          2.2070e-02, -4.0799e-03],\n",
      "        [ 3.9265e-03, -3.3373e-03,  6.8457e-03,  ..., -3.9414e-03,\n",
      "          6.7452e-03, -5.4771e-03],\n",
      "        ...,\n",
      "        [-6.0467e-03, -2.1862e-03,  6.9636e-03,  ..., -8.0257e-03,\n",
      "         -2.6812e-03, -4.0630e-03],\n",
      "        [-1.1771e-03,  1.3103e-03, -4.4129e-05,  ..., -8.8415e-04,\n",
      "         -6.2263e-04, -4.6925e-03],\n",
      "        [ 5.0481e-03,  1.5674e-02, -4.6409e-03,  ...,  3.6877e-03,\n",
      "          4.1789e-03, -3.3344e-03]])\n",
      "Weights are not tied for gpt_neox.layers.11.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0025, -0.0126, -0.0368,  ..., -0.0018, -0.0265,  0.0060],\n",
      "        [ 0.0380,  0.0225, -0.0237,  ...,  0.0038,  0.0147,  0.0058],\n",
      "        [-0.0423, -0.0077, -0.0016,  ...,  0.0087,  0.0216, -0.0002],\n",
      "        ...,\n",
      "        [-0.0090,  0.0190,  0.0051,  ..., -0.0293,  0.0436, -0.0172],\n",
      "        [-0.0242, -0.0113,  0.0408,  ...,  0.0514,  0.0237, -0.0019],\n",
      "        [ 0.0453,  0.0171,  0.0250,  ...,  0.0611, -0.0161,  0.0044]])\n",
      "Weights are not tied for gpt_neox.layers.11.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0020,  0.0130,  0.0032,  ..., -0.0048, -0.0047, -0.0066],\n",
      "        [-0.0107, -0.0062,  0.0177,  ..., -0.0010,  0.0127, -0.0021],\n",
      "        [ 0.0093,  0.0061, -0.0141,  ...,  0.0079,  0.0125, -0.0015],\n",
      "        ...,\n",
      "        [-0.0027,  0.0091,  0.0049,  ...,  0.0042,  0.0004, -0.0020],\n",
      "        [-0.0175,  0.0015, -0.0033,  ..., -0.0120,  0.0030, -0.0106],\n",
      "        [-0.0146,  0.0005,  0.0117,  ..., -0.0047,  0.0110, -0.0002]])\n",
      "Weights are not tied for gpt_neox.layers.12.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0027,  0.0109, -0.0192,  ..., -0.0011,  0.0340, -0.0322],\n",
      "        [ 0.0060, -0.0374, -0.0117,  ...,  0.0343,  0.0434, -0.0042],\n",
      "        [-0.0020,  0.0293,  0.0227,  ...,  0.0032,  0.0212, -0.0400],\n",
      "        ...,\n",
      "        [-0.0113,  0.0225, -0.0023,  ..., -0.0099, -0.0230, -0.0043],\n",
      "        [ 0.0225,  0.0007, -0.0006,  ..., -0.0472, -0.0171, -0.0057],\n",
      "        [ 0.0128, -0.0091, -0.0306,  ...,  0.0144,  0.0316,  0.0015]])\n",
      "Weights are not tied for gpt_neox.layers.12.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0070,  0.0246, -0.0255,  ...,  0.0101, -0.0031,  0.0043],\n",
      "        [ 0.0005, -0.0029,  0.0062,  ...,  0.0016, -0.0015,  0.0109],\n",
      "        [-0.0067, -0.0012,  0.0087,  ..., -0.0042, -0.0048,  0.0092],\n",
      "        ...,\n",
      "        [ 0.0100,  0.0034,  0.0011,  ...,  0.0058,  0.0005, -0.0022],\n",
      "        [-0.0026,  0.0103, -0.0062,  ...,  0.0072,  0.0023,  0.0030],\n",
      "        [-0.0047, -0.0103, -0.0122,  ...,  0.0148,  0.0058,  0.0055]])\n",
      "Weights are not tied for gpt_neox.layers.13.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0009, -0.0047,  0.0214,  ...,  0.0396, -0.0044, -0.0474],\n",
      "        [ 0.0024, -0.0217,  0.0379,  ..., -0.0057, -0.0562, -0.0228],\n",
      "        [ 0.0023, -0.0152, -0.0020,  ...,  0.0205,  0.0295, -0.0158],\n",
      "        ...,\n",
      "        [-0.0615,  0.0205,  0.0501,  ..., -0.0297,  0.0238, -0.0058],\n",
      "        [ 0.0176,  0.0287, -0.0200,  ...,  0.0202, -0.0016, -0.0012],\n",
      "        [ 0.0153, -0.0195,  0.0203,  ..., -0.0351, -0.0174,  0.0250]])\n",
      "Weights are not tied for gpt_neox.layers.13.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0005,  0.0014,  0.0099,  ..., -0.0135, -0.0063,  0.0035],\n",
      "        [ 0.0076,  0.0039,  0.0069,  ...,  0.0010, -0.0118,  0.0076],\n",
      "        [ 0.0011, -0.0015,  0.0083,  ...,  0.0023,  0.0019,  0.0078],\n",
      "        ...,\n",
      "        [ 0.0003,  0.0062, -0.0092,  ...,  0.0002,  0.0031, -0.0098],\n",
      "        [ 0.0049,  0.0070, -0.0054,  ...,  0.0017, -0.0038,  0.0100],\n",
      "        [ 0.0029,  0.0138, -0.0112,  ..., -0.0020, -0.0009,  0.0221]])\n",
      "Weights are not tied for gpt_neox.layers.14.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0080, -0.0057,  0.0299,  ...,  0.0158, -0.0068, -0.0272],\n",
      "        [ 0.0316, -0.0156,  0.0217,  ..., -0.0348, -0.0280, -0.0110],\n",
      "        [ 0.0284,  0.0022, -0.0017,  ...,  0.0056,  0.0210, -0.0172],\n",
      "        ...,\n",
      "        [ 0.0339, -0.0062, -0.0189,  ..., -0.0010, -0.0234, -0.0413],\n",
      "        [-0.0057, -0.0248, -0.0364,  ...,  0.0167,  0.0304, -0.0230],\n",
      "        [ 0.0279,  0.0077, -0.0156,  ...,  0.0358, -0.0042, -0.0050]])\n",
      "Weights are not tied for gpt_neox.layers.14.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0208,  0.0143, -0.0096,  ...,  0.0125,  0.0071, -0.0144],\n",
      "        [-0.0054,  0.0057, -0.0068,  ..., -0.0027, -0.0068, -0.0040],\n",
      "        [-0.0134, -0.0091,  0.0138,  ..., -0.0046, -0.0050,  0.0083],\n",
      "        ...,\n",
      "        [-0.0043, -0.0089, -0.0014,  ..., -0.0160, -0.0061, -0.0010],\n",
      "        [ 0.0017,  0.0012, -0.0001,  ...,  0.0153,  0.0087,  0.0010],\n",
      "        [ 0.0094, -0.0068,  0.0104,  ..., -0.0071, -0.0091, -0.0088]])\n",
      "Weights are not tied for gpt_neox.layers.15.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0247,  0.0091,  0.0181,  ..., -0.0029, -0.0287,  0.0169],\n",
      "        [ 0.0600, -0.0042, -0.0094,  ..., -0.0265, -0.0012,  0.0203],\n",
      "        [ 0.0102,  0.0066, -0.0062,  ...,  0.0143,  0.0127, -0.0228],\n",
      "        ...,\n",
      "        [ 0.0225,  0.0157,  0.0027,  ..., -0.0058, -0.0005,  0.0199],\n",
      "        [-0.0167, -0.0315,  0.0047,  ..., -0.0200, -0.0090, -0.0285],\n",
      "        [-0.0071, -0.0017,  0.0350,  ..., -0.0279,  0.0226,  0.0255]])\n",
      "Weights are not tied for gpt_neox.layers.15.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0029, -0.0130, -0.0108,  ..., -0.0094, -0.0052, -0.0040],\n",
      "        [-0.0016,  0.0015, -0.0001,  ..., -0.0027, -0.0069,  0.0118],\n",
      "        [-0.0023, -0.0075,  0.0023,  ...,  0.0023, -0.0031, -0.0003],\n",
      "        ...,\n",
      "        [-0.0021,  0.0069, -0.0076,  ..., -0.0071,  0.0014, -0.0021],\n",
      "        [-0.0074,  0.0127, -0.0012,  ..., -0.0076, -0.0052,  0.0110],\n",
      "        [-0.0011,  0.0067,  0.0008,  ..., -0.0096, -0.0008,  0.0001]])\n",
      "Weights are not tied for gpt_neox.layers.16.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0099,  0.0423, -0.0289,  ..., -0.0166,  0.0338, -0.0205],\n",
      "        [ 0.0088, -0.0157, -0.0042,  ..., -0.0206,  0.0181,  0.0179],\n",
      "        [-0.0155, -0.0270,  0.0397,  ...,  0.0110, -0.0174,  0.0031],\n",
      "        ...,\n",
      "        [-0.0418,  0.0173, -0.0505,  ..., -0.0518,  0.0356,  0.0469],\n",
      "        [-0.0350,  0.0186, -0.0230,  ..., -0.0208,  0.0198, -0.0287],\n",
      "        [-0.0106, -0.0212, -0.0025,  ...,  0.0006, -0.0242, -0.0159]])\n",
      "Weights are not tied for gpt_neox.layers.16.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-2.1761e-03,  2.4615e-03, -8.9961e-03,  ...,  4.3311e-05,\n",
      "          4.1501e-04, -7.3810e-03],\n",
      "        [-1.9867e-02,  6.9754e-03,  3.2740e-03,  ...,  2.4270e-03,\n",
      "          3.1715e-03,  2.3722e-04],\n",
      "        [ 2.0178e-02, -1.1433e-03,  2.2085e-03,  ...,  3.9046e-03,\n",
      "         -7.8172e-04, -5.1418e-03],\n",
      "        ...,\n",
      "        [ 9.2747e-04, -2.1816e-03,  7.4762e-03,  ...,  2.4304e-03,\n",
      "          5.1884e-03,  3.9567e-03],\n",
      "        [-5.9539e-03, -6.6231e-03,  1.6650e-03,  ..., -7.2316e-03,\n",
      "         -1.4478e-03,  9.2207e-03],\n",
      "        [ 1.0507e-02, -3.2208e-03,  2.4543e-02,  ...,  1.8276e-02,\n",
      "         -3.2238e-03, -2.6915e-03]])\n",
      "Weights are not tied for gpt_neox.layers.17.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0076,  0.0068,  0.0497,  ...,  0.0035,  0.0287,  0.0052],\n",
      "        [-0.0259,  0.0142,  0.0075,  ...,  0.0248, -0.0007,  0.0296],\n",
      "        [-0.0120, -0.0093,  0.0435,  ..., -0.0068,  0.0282, -0.0325],\n",
      "        ...,\n",
      "        [ 0.0166,  0.0356, -0.0107,  ..., -0.0006, -0.0006,  0.0179],\n",
      "        [ 0.0156, -0.0229, -0.0058,  ...,  0.0219, -0.0336, -0.0310],\n",
      "        [-0.0308,  0.0133,  0.0027,  ..., -0.0288, -0.0141,  0.0107]])\n",
      "Weights are not tied for gpt_neox.layers.17.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0077,  0.0085, -0.0204,  ...,  0.0134,  0.0172,  0.0036],\n",
      "        [-0.0043, -0.0135, -0.0061,  ..., -0.0027, -0.0081, -0.0059],\n",
      "        [ 0.0047,  0.0125,  0.0111,  ...,  0.0029,  0.0003,  0.0167],\n",
      "        ...,\n",
      "        [-0.0099, -0.0132,  0.0182,  ..., -0.0102, -0.0040,  0.0025],\n",
      "        [-0.0099, -0.0140, -0.0113,  ..., -0.0107, -0.0025, -0.0017],\n",
      "        [ 0.0056,  0.0129,  0.0189,  ...,  0.0079,  0.0031,  0.0022]])\n",
      "Weights are not tied for gpt_neox.layers.18.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0089,  0.0237,  0.0101,  ..., -0.0120, -0.0271,  0.0272],\n",
      "        [-0.0330, -0.0147, -0.0115,  ..., -0.0110,  0.0084, -0.0056],\n",
      "        [-0.0166,  0.0095,  0.0245,  ..., -0.0495, -0.0331,  0.0242],\n",
      "        ...,\n",
      "        [-0.0409, -0.0435, -0.0096,  ...,  0.0121,  0.0062,  0.0252],\n",
      "        [-0.0161,  0.0079, -0.0064,  ...,  0.0282, -0.0187,  0.0113],\n",
      "        [ 0.0272,  0.0038, -0.0221,  ...,  0.0186,  0.0140, -0.0204]])\n",
      "Weights are not tied for gpt_neox.layers.18.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0100,  0.0237, -0.0142,  ..., -0.0031,  0.0078, -0.0118],\n",
      "        [-0.0034, -0.0063,  0.0087,  ...,  0.0001,  0.0068, -0.0001],\n",
      "        [-0.0210, -0.0135, -0.0196,  ..., -0.0251,  0.0331, -0.0198],\n",
      "        ...,\n",
      "        [ 0.0035, -0.0146, -0.0140,  ..., -0.0073,  0.0025, -0.0069],\n",
      "        [ 0.0019,  0.0067,  0.0096,  ...,  0.0054, -0.0055,  0.0074],\n",
      "        [ 0.0026,  0.0076,  0.0031,  ...,  0.0077, -0.0073,  0.0073]])\n",
      "Weights are not tied for gpt_neox.layers.19.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0328,  0.0125,  0.0343,  ...,  0.0293, -0.0094,  0.0100],\n",
      "        [-0.0021, -0.0006,  0.0248,  ...,  0.0265, -0.0013,  0.0151],\n",
      "        [ 0.0274, -0.0244,  0.0150,  ...,  0.0330,  0.0293, -0.0296],\n",
      "        ...,\n",
      "        [-0.0138,  0.0025,  0.0123,  ..., -0.0074, -0.0129, -0.0013],\n",
      "        [-0.0060, -0.0292,  0.0200,  ...,  0.0179,  0.0164,  0.0173],\n",
      "        [-0.0021,  0.0314, -0.0008,  ..., -0.0173,  0.0124,  0.0068]])\n",
      "Weights are not tied for gpt_neox.layers.19.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0062, -0.0063,  0.0060,  ...,  0.0062, -0.0097, -0.0113],\n",
      "        [ 0.0158, -0.0001, -0.0101,  ..., -0.0097,  0.0091,  0.0113],\n",
      "        [ 0.0041,  0.0040, -0.0067,  ..., -0.0071,  0.0068,  0.0131],\n",
      "        ...,\n",
      "        [ 0.0014, -0.0035, -0.0017,  ..., -0.0002, -0.0020, -0.0033],\n",
      "        [ 0.0102, -0.0117,  0.0050,  ...,  0.0041, -0.0042, -0.0030],\n",
      "        [-0.0119,  0.0090,  0.0039,  ...,  0.0089, -0.0036, -0.0009]])\n",
      "Weights are not tied for gpt_neox.layers.20.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0097,  0.0088,  0.0074,  ..., -0.0265,  0.0096,  0.0118],\n",
      "        [ 0.0251, -0.0073,  0.0409,  ..., -0.0016, -0.0114, -0.0314],\n",
      "        [-0.0302, -0.0115,  0.0373,  ...,  0.0236, -0.0243, -0.0153],\n",
      "        ...,\n",
      "        [-0.0295, -0.0055, -0.0442,  ..., -0.0312,  0.0156,  0.0249],\n",
      "        [ 0.0221, -0.0152,  0.0058,  ...,  0.0010, -0.0378,  0.0150],\n",
      "        [ 0.0370,  0.0117, -0.0271,  ...,  0.0273,  0.0152, -0.0188]])\n",
      "Weights are not tied for gpt_neox.layers.20.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-6.7075e-03, -1.5559e-02,  1.1312e-02,  ..., -3.9506e-03,\n",
      "         -2.3101e-02,  2.7754e-03],\n",
      "        [-4.3741e-03,  1.0345e-02,  3.9773e-05,  ..., -5.9968e-04,\n",
      "          2.3875e-02,  5.7940e-03],\n",
      "        [ 2.6395e-03, -1.2967e-02,  7.0496e-03,  ..., -4.7641e-03,\n",
      "         -1.5373e-02,  4.3362e-03],\n",
      "        ...,\n",
      "        [-8.4520e-03,  2.9169e-03,  8.5807e-04,  ..., -2.4045e-02,\n",
      "         -9.8752e-03,  8.7406e-04],\n",
      "        [-3.2616e-04, -4.3837e-03,  3.9612e-03,  ...,  2.6553e-03,\n",
      "          6.2686e-03,  6.7758e-03],\n",
      "        [ 4.9137e-03, -7.0720e-03, -7.4061e-03,  ...,  7.6080e-03,\n",
      "         -9.9822e-04, -5.0707e-03]])\n",
      "Weights are not tied for gpt_neox.layers.21.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0003, -0.0182, -0.0068,  ..., -0.0215,  0.0184, -0.0075],\n",
      "        [-0.0087,  0.0150, -0.0125,  ...,  0.0028, -0.0153, -0.0178],\n",
      "        [-0.0052,  0.0106, -0.0184,  ..., -0.0077, -0.0171,  0.0019],\n",
      "        ...,\n",
      "        [-0.0179,  0.0178,  0.0082,  ...,  0.0025, -0.0236, -0.0186],\n",
      "        [ 0.0240,  0.0097, -0.0234,  ...,  0.0252, -0.0242,  0.0026],\n",
      "        [-0.0181,  0.0145,  0.0166,  ..., -0.0011, -0.0299, -0.0122]])\n",
      "Weights are not tied for gpt_neox.layers.21.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0036,  0.0033, -0.0094,  ..., -0.0004, -0.0081, -0.0047],\n",
      "        [ 0.0068,  0.0120, -0.0172,  ...,  0.0013, -0.0044, -0.0120],\n",
      "        [-0.0008,  0.0081, -0.0112,  ...,  0.0063, -0.0202, -0.0067],\n",
      "        ...,\n",
      "        [ 0.0022,  0.0005, -0.0070,  ...,  0.0011, -0.0004, -0.0021],\n",
      "        [-0.0017,  0.0063,  0.0091,  ..., -0.0035,  0.0056,  0.0041],\n",
      "        [ 0.0072, -0.0011, -0.0037,  ...,  0.0046,  0.0019, -0.0096]])\n",
      "Weights are not tied for gpt_neox.layers.22.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0082, -0.0339,  0.0234,  ...,  0.0351,  0.0115, -0.0217],\n",
      "        [-0.0165,  0.0167,  0.0123,  ..., -0.0395,  0.0107,  0.0196],\n",
      "        [ 0.0101,  0.0169, -0.0026,  ...,  0.0204,  0.0190, -0.0035],\n",
      "        ...,\n",
      "        [-0.0239,  0.0329,  0.0098,  ..., -0.0218,  0.0367,  0.0197],\n",
      "        [ 0.0128, -0.0265, -0.0276,  ..., -0.0203, -0.0313, -0.0207],\n",
      "        [ 0.0130,  0.0030,  0.0074,  ..., -0.0055, -0.0086, -0.0097]])\n",
      "Weights are not tied for gpt_neox.layers.22.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 7.3962e-03, -1.2720e-02,  6.5627e-03,  ...,  1.2591e-02,\n",
      "          1.3571e-02,  6.9853e-03],\n",
      "        [-6.0679e-03,  1.2597e-02, -1.2393e-03,  ..., -8.8962e-03,\n",
      "         -8.3864e-03,  1.5415e-03],\n",
      "        [ 1.6853e-02, -2.0670e-02,  1.4933e-02,  ...,  2.3500e-02,\n",
      "          1.7845e-02,  2.9908e-02],\n",
      "        ...,\n",
      "        [-2.1883e-02,  2.5568e-02,  5.0575e-03,  ..., -6.4077e-03,\n",
      "         -1.2355e-03, -3.4924e-03],\n",
      "        [ 2.9439e-03, -9.6719e-03,  1.2800e-04,  ..., -9.1092e-05,\n",
      "         -1.0982e-03,  6.0449e-03],\n",
      "        [-4.5522e-03,  1.8362e-02, -3.9553e-03,  ..., -1.2284e-02,\n",
      "         -1.6039e-02, -5.8448e-03]])\n",
      "Weights are not tied for gpt_neox.layers.23.attention.query_key_value.lora_A.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[ 0.0210,  0.0187,  0.0049,  ...,  0.0025,  0.0133,  0.0180],\n",
      "        [-0.0118, -0.0235,  0.0004,  ..., -0.0393,  0.0078,  0.0063],\n",
      "        [-0.0015,  0.0152, -0.0135,  ..., -0.0090,  0.0202, -0.0365],\n",
      "        ...,\n",
      "        [-0.0171, -0.0081,  0.0052,  ...,  0.0284,  0.0275,  0.0315],\n",
      "        [-0.0179, -0.0218,  0.0163,  ..., -0.0100,  0.0163, -0.0117],\n",
      "        [ 0.0189,  0.0093, -0.0204,  ..., -0.0268, -0.0151,  0.0136]])\n",
      "Weights are not tied for gpt_neox.layers.23.attention.query_key_value.lora_B.default.weight!\n",
      "Parameter containing:\n",
      "tensor([[-0.0146,  0.0055, -0.0140,  ..., -0.0106, -0.0163,  0.0144],\n",
      "        [ 0.0120,  0.0005,  0.0111,  ...,  0.0059,  0.0125, -0.0064],\n",
      "        [-0.0152,  0.0123,  0.0020,  ..., -0.0149, -0.0170,  0.0113],\n",
      "        ...,\n",
      "        [-0.0017, -0.0032,  0.0063,  ..., -0.0020, -0.0018, -0.0014],\n",
      "        [-0.0003, -0.0069, -0.0067,  ...,  0.0060,  0.0095,  0.0033],\n",
      "        [-0.0021,  0.0130,  0.0018,  ..., -0.0141, -0.0140,  0.0050]])\n"
     ]
    }
   ],
   "source": [
    "# assert that all the lora_ weights are the same\n",
    "\n",
    "first_layer_lora_A = model_small.gpt_neox.layers[0].attention.query_key_value.lora_A.default.weight\n",
    "first_layer_lora_B = model_small.gpt_neox.layers[0].attention.query_key_value.lora_B.default.weight\n",
    "\n",
    "print(f\"First layer A: {first_layer_lora_A}\")\n",
    "print(f\"First layer B: {first_layer_lora_B}\")\n",
    "\n",
    "for name, param in model_small.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        if not torch.equal(param, first_layer_lora_A) and not torch.equal(param, first_layer_lora_B):\n",
    "            print(f\"Weights are not tied for {name}!\")\n",
    "            print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy with same rank\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=args.rank, \n",
    "    lora_alpha=args.lora_alpha,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model_large = get_peft_model(model_large, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_in_a, new_out_a = 1024, args.rank   # 512, 768, 1024\n",
    "# new_in_b, new_out_b = args.rank, 3072  # ? , 2304, 3072\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(50304, 2048)\n",
       "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (attention): GPTNeoXAttention(\n",
       "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=2048, out_features=50304, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the in_features and out_features of qkv\n",
    "\n",
    "model_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand module\n",
    "\n",
    "def expand_lora_copy(old_module, new_in, new_out):\n",
    "    new_module = nn.Linear(new_in, new_out, bias=old_module.bias is not None)\n",
    "    nn.init.zeros_(new_module.weight)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        new_module.weight[:old_module.out_features, :old_module.in_features].copy_(old_module.weight)\n",
    "        \n",
    "        for i in range(old_module.in_features, new_in):\n",
    "            new_module.weight[:old_module.out_features, i].copy_(old_module.weight[:, i % old_module.in_features])\n",
    "        \n",
    "        for j in range(old_module.out_features, new_out):\n",
    "            new_module.weight[j, :old_module.in_features].copy_(old_module.weight[j % old_module.out_features, :])\n",
    "    \n",
    "    if old_module.bias is not None:\n",
    "        new_module.bias[:old_module.out_features].copy_(old_module.bias)\n",
    "    \n",
    "    return new_module\n",
    "\n",
    "\n",
    "def expand_lora_padding(old_module, new_in, new_out):\n",
    "    new_module = nn.Linear(new_in, new_out, bias=old_module.bias is not None)\n",
    "    nn.init.zeros_(new_module.weight)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        new_module.weight[:old_module.out_features, :old_module.in_features].copy_(old_module.weight)\n",
    "\n",
    "    if old_module.bias is not None:\n",
    "        new_module.bias[:old_module.out_features].copy_(old_module.bias)\n",
    "    \n",
    "    return new_module\n",
    "\n",
    "def expand_lora_normal(old_module, new_in, new_out):\n",
    "    new_module = nn.Linear(new_in, new_out, bias=old_module.bias is not None)\n",
    "    \n",
    "    # Initialize with Kaiming-uniform initialization\n",
    "    nn.init.kaiming_uniform_(new_module.weight, a=math.sqrt(5))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        new_module.weight[:old_module.out_features, :old_module.in_features].copy_(old_module.weight)\n",
    "\n",
    "    if old_module.bias is not None:\n",
    "        new_module.bias[:old_module.out_features].copy_(old_module.bias)\n",
    "\n",
    "    return new_module\n",
    "\n",
    "def expand_lora_noop_normal(old_module, new_in, new_out):\n",
    "    new_module = nn.Linear(new_in, new_out, bias=old_module.bias is not None)\n",
    "    \n",
    "    # Initialize with Kaiming-uniform initialization\n",
    "    nn.init.kaiming_uniform_(new_module.weight, a=math.sqrt(5))\n",
    "\n",
    "    return new_module\n",
    "\n",
    "def expand_lora_noop_zero(old_module, new_in, new_out):\n",
    "    new_module = nn.Linear(new_in, new_out, bias=old_module.bias is not None)\n",
    "    \n",
    "    # Initialize with zeros\n",
    "    nn.init.zeros_(new_module.weight)\n",
    "\n",
    "    return new_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the LoRA adapter for larger model\n",
    "\n",
    "# config_lora = LoraConfig(\n",
    "#     r=64, \n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.05,\n",
    "#     target_modules=[\"query_key_value\"],\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "# model_large_ft = get_peft_model(model_large_pt, config_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_in_a = model_large.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the feature sizes and expansion method here\n",
    "\n",
    "\n",
    "for name, module in model_small.named_modules():\n",
    "    if name.endswith(\"lora_A\"):\n",
    "        new_module = expand_lora_padding(module.default, model_large.config.hidden_size, args.rank)\n",
    "        parts = name.split('.')\n",
    "        parent_module = model_large\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        \n",
    "        setattr(parent_module, parts[-1], nn.ModuleDict({\"default\": new_module}))\n",
    "\n",
    "    elif name.endswith(\"lora_B\"):\n",
    "        new_module = expand_lora_padding(module.default, args.rank, model_large.config.hidden_size * 3)\n",
    "        parts = name.split('.')\n",
    "        parent_module = model_large\n",
    "        for part in parts[:-1]:\n",
    "            parent_module = getattr(parent_module, part)\n",
    "        \n",
    "        setattr(parent_module, parts[-1], nn.ModuleDict({\"default\": new_module}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unwrapped_model = accelerator.unwrap_model(model)\n",
    "#             unwrapped_model.save_pretrained(\n",
    "#                 output,\n",
    "#                 is_main_process=accelerator.is_main_process,\n",
    "#                 save_function=accelerator.save,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the expanded model\n",
    "\n",
    "model_large.save_pretrained(args.expanded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\"./weight/pythia_{args.small_model}_{args.large_model}_{args.expand_method}_r=64_schedule/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
